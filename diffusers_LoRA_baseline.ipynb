{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "121uKqYjkR4qWpVU9lS-JTPiuViDmPDKp",
      "authorship_tag": "ABX9TyP/zoSlIGpXKsylZoBe6anw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taiki738/diffusers/blob/main/diffusers_LoRA_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. Setup & Environment\n",
        "# ==========================================\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Driveã®ãƒã‚¦ãƒ³ãƒˆ\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# è¨­å®š: ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‘ã‚¹\n",
        "REPO_DIR = \"/content/drive/MyDrive/github/diffusers\"\n",
        "\n",
        "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã¯ã‚¯ãƒ­ãƒ¼ãƒ³ã€ã‚ã‚‹å ´åˆã¯æ›´æ–°\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(f\"ğŸ“¥ Cloning repository to {REPO_DIR}...\")\n",
        "    !git clone https://github.com/taiki738/diffusers.git {REPO_DIR}\n",
        "else:\n",
        "    print(f\"ğŸ”„ Updating repository in {REPO_DIR}...\")\n",
        "    os.chdir(REPO_DIR)\n",
        "    !git pull origin main\n",
        "\n",
        "# ãƒªãƒã‚¸ãƒˆãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ç§»å‹•\n",
        "os.chdir(REPO_DIR)\n",
        "print(f\"ğŸ“‚ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "print(\"â³ Installing dependencies...\")\n",
        "!pip install -e .\n",
        "!pip install -q accelerate transformers ftfy xformers\n",
        "\n",
        "print(\"âœ… Setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x8n3F_3PB4R",
        "outputId": "563dbd3a-f465-40ba-ad30-700e6fa59f01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ğŸ”„ Updating repository in /content/drive/MyDrive/github/diffusers...\n",
            "From https://github.com/taiki738/diffusers\n",
            " * branch                main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "ğŸ“‚ Current directory: /content/drive/MyDrive/github/diffusers\n",
            "â³ Installing dependencies...\n",
            "Obtaining file:///content/drive/MyDrive/github/diffusers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (8.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (3.20.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (0.36.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (0.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers==0.37.0.dev0) (11.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.37.0.dev0) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.37.0.dev0) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.37.0.dev0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.37.0.dev0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->diffusers==0.37.0.dev0) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (6.0.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.37.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers==0.37.0.dev0) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.37.0.dev0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.37.0.dev0) (2.5.0)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building editable for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.37.0.dev0-0.editable-py3-none-any.whl size=11392 sha256=c42b8a613857bbcbe11a27d9620a2550eaece36320f90274a3333dd8e3d03478\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ioweuw8c/wheels/cb/0c/d2/eee942993f14cc21e8ffdade5414c04891d0c121a68dd6bf14\n",
            "Successfully built diffusers\n",
            "Installing collected packages: diffusers\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.36.0\n",
            "    Uninstalling diffusers-0.36.0:\n",
            "      Successfully uninstalled diffusers-0.36.0\n",
            "Successfully installed diffusers-0.37.0.dev0\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m155.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. Dataset Preparation\n",
        "# ==========================================\n",
        "# è¨­å®š: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‘ã‚¹ã¨å±•é–‹å…ˆ\n",
        "DATA_ARCHIVE_PATH = \"/content/drive/MyDrive/github/latent-diffusion/data/ffhq_survey.tar.gz\"\n",
        "EXTRACT_DIR = \"/content/ffhq_for_survey\"\n",
        "\n",
        "print(f\"â³ Extracting dataset from {DATA_ARCHIVE_PATH} to {EXTRACT_DIR}...\")\n",
        "\n",
        "# æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "!rm -rf {EXTRACT_DIR}\n",
        "\n",
        "# ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®è§£å‡ (å®Ÿä½“ã¨ã—ã¦å±•é–‹)\n",
        "!tar -xhzf \"{DATA_ARCHIVE_PATH}\" -C /content/\n",
        "\n",
        "# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (metadata.jsonl) ã®ç”Ÿæˆ\n",
        "# pushæ¸ˆã¿ã®ãƒªãƒã‚¸ãƒˆãƒªå†…ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨\n",
        "print(\"â³ Generating metadata...\")\n",
        "!python scripts/prepare_lora_colab.py --dataset_root {EXTRACT_DIR}\n",
        "\n",
        "print(\"âœ… Dataset ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_82zr5pBPyi9",
        "outputId": "5af4e93d-eee1-4564-f8ed-15ca887bffbf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Extracting dataset from /content/drive/MyDrive/github/latent-diffusion/data/ffhq_survey.tar.gz to /content/ffhq_for_survey...\n",
            "â³ Generating metadata...\n",
            "Scanning directory: /content/ffhq_for_survey\n",
            "  - male/OK_4.0: 76 images found.\n",
            "  - male/not-OK_2.0: 190 images found.\n",
            "  - female/OK_4.0: 131 images found.\n",
            "  - female/not-OK_2.0: 155 images found.\n",
            "\n",
            "Success! Created 552 entries in /content/ffhq_for_survey/metadata.jsonl\n",
            "You can now start training with --train_data_dir pointing to the dataset_root.\n",
            "âœ… Dataset ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. Start LoRA Training\n",
        "# ==========================================\n",
        "# è¨­å®š: å­¦ç¿’çµæœã®å‡ºåŠ›å…ˆ (Driveä¸Šæ¨å¥¨)\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/github/diffusers/outputs/lora_male_v1\"\n",
        "\n",
        "print(f\"ğŸš€ Starting training... Output will be saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "# å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œ\n",
        "!accelerate launch examples/text_to_image/train_text_to_image_lora.py \\\n",
        "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
        "  --train_data_dir=\"{EXTRACT_DIR}\" \\\n",
        "  --resolution=512 \\\n",
        "  --random_flip \\\n",
        "  --train_batch_size=4 \\\n",
        "  --num_train_epochs=100 \\\n",
        "  --checkpointing_steps=500 \\\n",
        "  --learning_rate=1e-04 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --seed=42 \\\n",
        "  --rank=4 \\\n",
        "  --output_dir=\"{OUTPUT_DIR}\" \\\n",
        "  --validation_prompt=\"a photo of a male face, high score impression\" \\\n",
        "  --report_to=\"tensorboard\" \\\n",
        "  --resume_from_checkpoint=\"latest\"\n",
        "\n",
        "print(\"ğŸ‰ Training process finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXEzPW9tQpNA",
        "outputId": "e2fe6afd-2e9d-4944-fb01-825b16027ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting training... Output will be saved to: /content/drive/MyDrive/github/diffusers/outputs/lora_male_v1\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2026-01-13 02:55:53.927905: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-13 02:55:53.944884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768272953.968294    2632 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768272953.975777    2632 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768272953.995204    2632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768272953.995237    2632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768272953.995240    2632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768272953.995243    2632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-13 02:55:54.000144: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "INFO:__main__:Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "scheduler_config.json: 100% 308/308 [00:00<00:00, 1.93MB/s]\n",
            "{'sample_max_value', 'dynamic_thresholding_ratio', 'prediction_type', 'rescale_betas_zero_snr', 'variance_type', 'timestep_spacing', 'thresholding', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
            "tokenizer_config.json: 100% 806/806 [00:00<00:00, 5.31MB/s]\n",
            "vocab.json: 1.06MB [00:00, 85.5MB/s]\n",
            "merges.txt: 525kB [00:00, 137MB/s]\n",
            "special_tokens_map.json: 100% 472/472 [00:00<00:00, 4.59MB/s]\n",
            "config.json: 100% 617/617 [00:00<00:00, 4.03MB/s]\n",
            "text_encoder/model.safetensors: 100% 492M/492M [00:02<00:00, 197MB/s]\n",
            "config.json: 100% 547/547 [00:00<00:00, 4.18MB/s]\n",
            "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 263MB/s]\n",
            "{'force_upcast', 'latents_std', 'latents_mean', 'mid_block_add_attention', 'scaling_factor', 'shift_factor', 'use_quant_conv', 'use_post_quant_conv'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "config.json: 100% 743/743 [00:00<00:00, 5.50MB/s]\n",
            "unet/diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [00:09<00:00, 359MB/s]\n",
            "{'cross_attention_norm', 'time_embedding_act_fn', 'time_embedding_dim', 'num_class_embeds', 'reverse_transformer_layers_per_block', 'dual_cross_attention', 'addition_embed_type', 'addition_time_embed_dim', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'timestep_post_act', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'mid_block_type', 'num_attention_heads', 'attention_type', 'time_cond_proj_dim', 'transformer_layers_per_block', 'time_embedding_type', 'only_cross_attention', 'use_linear_projection', 'dropout', 'encoder_hid_dim', 'encoder_hid_dim_type', 'resnet_skip_time_act', 'addition_embed_type_num_heads', 'class_embed_type', 'conv_out_kernel', 'upcast_attention', 'class_embeddings_concat', 'resnet_time_scale_shift'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "Resolving data files: 100% 553/553 [00:00<00:00, 35415.24it/s]\n",
            "Downloading data: 100% 553/553 [00:00<00:00, 32538.62files/s]\n",
            "Generating train split: 552 examples [00:00, 17716.72 examples/s]\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 552\n",
            "INFO:__main__:  Num Epochs = 100\n",
            "INFO:__main__:  Instantaneous batch size per device = 4\n",
            "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "INFO:__main__:  Gradient Accumulation steps = 1\n",
            "INFO:__main__:  Total optimization steps = 13800\n",
            "Resuming from checkpoint checkpoint-13500\n",
            "INFO:accelerate.accelerator:Loading states from /content/drive/MyDrive/github/diffusers/outputs/lora_male_v1/checkpoint-13500\n",
            "INFO:accelerate.checkpointing:All model weights loaded successfully\n",
            "INFO:accelerate.checkpointing:All optimizer states loaded successfully\n",
            "INFO:accelerate.checkpointing:All scheduler states loaded successfully\n",
            "INFO:accelerate.checkpointing:All dataloader sampler states loaded successfully\n",
            "INFO:accelerate.checkpointing:All random states loaded successfully\n",
            "INFO:accelerate.accelerator:Loading in 0 custom states\n",
            "Steps:  98% 13525/13800 [00:40<07:17,  1.59s/it, lr=0.0001, step_loss=0.175]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSFZ5cMBUbRi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}