import argparse
import os
import json
import numpy as np
import cv2
from tqdm import tqdm
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity

# insightfaceã¯ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦
# !pip install insightface onnxruntime-gpu

def get_image_paths(folder):
    valid_extensions = ('.png', '.jpg', '.jpeg', '.webp')
    return [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(valid_extensions)]

def extract_features(app, image_paths, desc="Extracting features"):
    features = []
    valid_paths = []
    
    for path in tqdm(image_paths, desc=desc):
        img = cv2.imread(path)
        if img is None:
            continue
            
        faces = app.get(img)
        
        # é¡”ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿æŽ¡ç”¨
        # è¤‡æ•°ã‚ã‚‹å ´åˆã¯ä¸€ç•ªå¤§ãã„é¡”ã‚’æŽ¡ç”¨
        if len(faces) > 0:
            # é¢ç©ã§ã‚½ãƒ¼ãƒˆ
            faces.sort(key=lambda x: (x.bbox[2]-x.bbox[0]) * (x.bbox[3]-x.bbox[1]), reverse=True)
            embedding = faces[0].embedding # 512æ¬¡å…ƒ
            features.append(embedding)
            valid_paths.append(path)
            
    if not features:
        return None, []
        
    # æ­£è¦åŒ– (Cosine Similarityç”¨)
    features = np.array(features)
    norms = np.linalg.norm(features, axis=1, keepdims=True)
    normalized_features = features / norms
    
    return normalized_features, valid_paths

def main():
    parser = argparse.ArgumentParser(description="Evaluate LoRA models using ArcFace (Similarity & IRS).")
    parser.add_argument("--train_dir", type=str, required=True, help="Path to training images (Reference for Similarity & IRS)")
    parser.add_argument("--gen_dir", type=str, required=True, help="Path to generated images (Target)")
    parser.add_argument("--output_json", type=str, default="evaluation_arcface.json", help="Path to save results")
    parser.add_argument("--irs_threshold", type=float, default=0.75, help="Threshold for identity copy detection (IRS)")
    
    args = parser.parse_args()
    
    try:
        import insightface
        from insightface.app import FaceAnalysis
    except ImportError:
        print("âŒ Error: insightface not installed. Please run: pip install insightface onnxruntime-gpu")
        return

    print("ðŸš€ Initializing FaceAnalysis (ArcFace)...")
    app = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
    app.prepare(ctx_id=0, det_size=(640, 640))
    
    # 1. ç‰¹å¾´é‡æŠ½å‡º
    train_paths = get_image_paths(args.train_dir)
    gen_paths = get_image_paths(args.gen_dir)
    
    print(f"ðŸ“Š Analyzing Train Data: {len(train_paths)} images")
    train_feats, _ = extract_features(app, train_paths, desc="Train Embeddings")
    
    print(f"ðŸ“Š Analyzing Gen Data: {len(gen_paths)} images")
    gen_feats, _ = extract_features(app, gen_paths, desc="Gen Embeddings")
    
    if train_feats is None or gen_feats is None:
        print("âŒ Error: Could not extract features from one of the directories.")
        return

    results = {}

    # 2. ArcFace Cosine Similarity (å±žæ€§ã®è¿‘ã•)
    # Trainãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®ã€Œå¹³å‡é¡”ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆé‡å¿ƒï¼‰ã€ã‚’è¨ˆç®—
    print("\nðŸ“ Calculating Attribute Similarity...")
    train_center = np.mean(train_feats, axis=0)
    train_center = train_center / np.linalg.norm(train_center) # å†æ­£è¦åŒ–
    
    # ç”Ÿæˆç”»åƒãã‚Œãžã‚Œã¨ã€Trainé‡å¿ƒã¨ã®é¡žä¼¼åº¦ã‚’è¨ˆç®—
    # (ç”Ÿæˆç”»åƒãŒã€ŒTrainãƒ‡ãƒ¼ã‚¿ã®å¹³å‡çš„ãªé¡”ï¼ˆå¥½å°è±¡é¡”ï¼‰ã€ã«ã©ã‚Œã ã‘ä¼¼ã¦ã„ã‚‹ã‹)
    sims_to_center = np.dot(gen_feats, train_center)
    avg_sim = np.mean(sims_to_center)
    
    print(f"âœ… ArcFace Similarity (vs Train Center): {avg_sim:.4f}")
    results["arcface_similarity"] = float(avg_sim)

    # 3. IRS / Nearest Neighbor Distance (éŽå­¦ç¿’ãƒã‚§ãƒƒã‚¯)
    # ç”Ÿæˆç”»åƒ1æžš1æžšã«ã¤ã„ã¦ã€Trainãƒ‡ãƒ¼ã‚¿ã®ä¸­ã§ã€Œä¸€ç•ªä¼¼ã¦ã„ã‚‹ã‚„ã¤ã€ã‚’æŽ¢ã™
    print("\nðŸ•µï¸ Calculating IRS (Overfitting Check)...")
    
    # è¡Œåˆ—æ¼”ç®—ã§å…¨å¯¾å…¨ã®é¡žä¼¼åº¦ã‚’ä¸€æ‹¬è¨ˆç®— (Gen x Train)
    # sim_matrix[i, j] = Gen[i] ã¨ Train[j] ã®é¡žä¼¼åº¦
    sim_matrix = np.dot(gen_feats, train_feats.T)
    
    # å„ç”Ÿæˆç”»åƒã”ã¨ã®æœ€å¤§é¡žä¼¼åº¦ (Nearest Neighbor Similarity)
    max_sims = np.max(sim_matrix, axis=1)
    
    # é–¾å€¤ã‚’è¶…ãˆãŸå‰²åˆ (IRS)
    overfit_count = np.sum(max_sims > args.irs_threshold)
    irs_score = overfit_count / len(gen_feats)
    avg_max_sim = np.mean(max_sims)
    
    print(f"âœ… Average Max Similarity: {avg_max_sim:.4f}")
    print(f"âš ï¸ IRS (Copy Rate > {args.irs_threshold}): {irs_score:.2%} ({overfit_count}/{len(gen_feats)})")
    
    results["irs_score"] = float(irs_score)
    results["avg_max_similarity"] = float(avg_max_sim)
    results["config"] = {
        "train_dir": args.train_dir,
        "gen_dir": args.gen_dir,
        "irs_threshold": args.irs_threshold
    }
    
    # Save
    with open(args.output_json, "w") as f:
        json.dump(results, f, indent=4)
        
    print(f"\nðŸŽ‰ Analysis finished. Results saved to {args.output_json}")

if __name__ == "__main__":
    main()
